{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# n-gram Language Model\n",
    "Here we will explore how to build an n-gram language model. n-gram language models are one of primitive types of language modelling done using conditional probablity approach. Watch this video to understand more: https://www.youtube.com/watch?v=iWea12EAu6U&list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z&index=6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The idea\n",
    "We will build a simple sentence completion model. This model will first read through a piece of novel then generate texts based on seed word(s) using a probablistic approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method to read the text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file_path: str) -> str:\n",
    "    \"\"\"\n",
    "    This function reads a text file and returns the string.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_path : str\n",
    "        The complete text file path\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        The content of the text file\n",
    "\n",
    "    \"\"\"\n",
    "    with open(file_path,'r',encoding='utf-8') as file:\n",
    "        s = file.read()\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Harry Potter and the Sorcerer's Stone \\n\\nCHAPTER ONE \\n\\nTHE BOY WHO LIVED \\n\\nMr. and Mrs. Dursley, of number four, Privet Drive, were proud to say that they were perfectly normal, thank you very much. They were the last people you'd expect to be involved in anything strange or mysterious, because they just didn't hold with such nonsense. \\n\\nMr. Dursley was the director of a firm called Grunnings, which made drills. He was a big, beefy man with hardly any neck, although he did have a very large mustache. Mrs. Dursley was thin and blonde and had nearly twice the usual amount of neck, which came in very useful as she spent so much of her time craning over garden fences, spying on the neighbors. The Dursleys had a small son called Dudley and in their opinion there was no finer boy anywhere. \\n\\nThe Dursleys had everything they wanted, but they also had a secret, and their greatest fear was that somebody would discover it. They didn't think they could bear it if anyone found out about the Potters\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = './datasets/Harry Potter and the Sorcerer.txt'\n",
    "s = read_file(file_path)\n",
    "s[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method to clean text\n",
    "Text cleaning is a major challenge in NLP tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    '''\n",
    "    Cleans the text by\n",
    "    1. Changing the end of sentence tokens to add space between them and the words.\n",
    "    2. Other special characters to be removed\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        Unclean text\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        Cleaned text\n",
    "\n",
    "    '''\n",
    "    # expand contractions\n",
    "    def decontracted(phrase):\n",
    "        # specific\n",
    "        phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "        phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "        # general\n",
    "        phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "        phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "        phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "        phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "        phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "        phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "        phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "        phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "        return phrase\n",
    "    text = decontracted(text)\n",
    "    \n",
    "    # create a transformation dictionary to put a space before the special characters encountered\n",
    "    transform_dict = {}\n",
    "    for chars in string.punctuation:\n",
    "        transform_dict[chars] = ' '+chars+' '\n",
    "    text = text.translate(str.maketrans(transform_dict))\n",
    "    \n",
    "    # replace multiple line feeds with a single space\n",
    "    text = re.sub('\\n+',' ', text)\n",
    "    # replace multiple spaces with a single space\n",
    "    text = re.sub(' +',' ', text)\n",
    "    # lower case everything\n",
    "    text = text.lower()\n",
    "    # remove all characters other than '.','?','!', ',' and'\"'\n",
    "    text = text.translate(str.maketrans('','','#$%&\\'()*+-/:;<=>@[\\\\]^_`{|}~'))\n",
    "    # strip the sentence\n",
    "    text = text.strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'harry potter and the sorcerer is stone chapter one the boy who lived mr . and mrs . dursley , of number four , privet drive , were proud to say that they were perfectly normal , thank you very much . they were the last people you would expect to be involved in anything strange or mysterious , because they just did not hold with such nonsense . mr . dursley was the director of a firm called grunnings , which made drills . he was a big , beefy man with hardly any neck , although he did have a very large mustache . mrs . dursley was thin and blonde and had nearly twice the usual amount of neck , which came in very useful as she spent so much of her time craning over garden fences , spying on the neighbors . the dursleys had a small son called dudley and in their opinion there was no finer boy anywhere . the dursleys had everything they wanted , but they also had a secret , and their greatest fear was that somebody would discover it . they did not think they could bear it if anyone found o'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = clean_text(text=s)\n",
    "text[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Idea\n",
    "The idea will be as follows:\n",
    "1. User will enter a seed word or series of words and model will predict the next word.\n",
    "1. If its a series of words it should count the occurence of last 4-gram, if not found it will calculate occurance of 3-gram, 2-gram, 1-gram and so on.\n",
    "\n",
    "One such E.g.:<br>\n",
    "Input: harry potter <br>\n",
    "Output: harry potter is ... <br>\n",
    "In other words we have to find the below probabilities:<br>\n",
    "P(is|harry potter) = p(is ∩ (harry, potter))/p(harry, potter)<br>\n",
    "If p(harry, potter) = 0 <br>\n",
    "Find p(is ∩ (potter))/p(potter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate vocabulary\n",
    "This method will generate the vocabulary out of the given text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_vocab(\n",
    "    text_corpus: str\n",
    "              ) -> list:\n",
    "    \"\"\"\n",
    "    Generate the vocabulary\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text_corpus : str\n",
    "        The whole text\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        Sorted list of unique words that appeared in our corpus.\n",
    "\n",
    "    \"\"\"\n",
    "    words = text_corpus.split(' ')\n",
    "    words = list(set(words))\n",
    "    words.sort()\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'is', 'it', 'test', 'this']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = generate_vocab(text_corpus='this is a test is it a test')\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search count of phrase occurances in corpus\n",
    "Below method will calculate the number of times the given text sequence appeared in our corpus.\n",
    "1. First split the corpus words {this|is|a|test|is|it}\n",
    "1. Split the search phrase words {is|a}\n",
    "1. scan each word in the given corpus and with each word searched check whether it matches the first position of search phrase word list i.e. 'is'.\n",
    "1. If a match is found start a loop within the words given in search phrase.\n",
    "1. With each word in the search phrase check whether the same words sequence appear together in corpus words list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_gram(\n",
    "    search_phrase: str,\n",
    "    text_corpus: str\n",
    "    ) -> int:\n",
    "    '''\n",
    "    This method will count number of times the seed text appeared together in the text\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    search_phrase : str\n",
    "        The text to search.\n",
    "    text_corpus : str\n",
    "        The cleaned text corpus where to search the seed text.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        DESCRIPTION.\n",
    "\n",
    "    '''\n",
    "    corpus_words = text_corpus.split(' ')\n",
    "    search_phrase_words = search_phrase.split(' ')\n",
    "    count = 0\n",
    "    for i,word in enumerate(corpus_words):\n",
    "        found: False\n",
    "        # search only if first word from the seed_text_words matches the given corpus word scan\n",
    "        if search_phrase_words[0] == word:\n",
    "            # search for the rest of the seed text words whether it is appearing in corpus words\n",
    "            # at the same sequence\n",
    "            for j, seed_word in enumerate(search_phrase_words):\n",
    "                if corpus_words[i+j] == seed_word:\n",
    "                    found = True\n",
    "                else:\n",
    "                    # if the corpus word mismatches the given sequence word break the loop\n",
    "                    found = False\n",
    "                    break\n",
    "            if found:\n",
    "#                 print('Sequence found in positions: {}'.format(i))\n",
    "                count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_of_text = count_gram(search_phrase='harry potter', text_corpus=text)\n",
    "count_of_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess the seed text\n",
    "This method will make sure we are looking at a maximum of 4 gram text sequence. <br>\n",
    "Input: this is a beautifully constructed long text which we do not want <br>\n",
    "Output: we do not want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_seed_text(\n",
    "    seed_text: str,\n",
    "    n_gram: int = 4\n",
    ")-> str:\n",
    "    '''\n",
    "    This method will make sure the we are looking at maximum of a 4-gram search\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    seed_text : str\n",
    "        The seed_text as entered by user\n",
    "    n_gram : int (optional)\n",
    "        N-grams to process.\n",
    "        Default value 4\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        The truncated 4-gram seed text\n",
    "\n",
    "    '''\n",
    "    words = seed_text.split(' ')\n",
    "    words = words[-n_gram:]\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'not want'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_seed_text(seed_text='this is a beautifully constructed long text which we do not want',\n",
    "                     n_gram = 2\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computation of probabilities\n",
    "Here we will compute the following probabilities<br>\n",
    "Suppose we want to generate a text as follows: 'harry potter is a wizard _____'. We need to go on computing the probabilities as follows:\n",
    "1. p(is ∩ (harry, potter))/p(harry, potter)<br>\n",
    "   = count(harry potter is)/count(harry potter)\n",
    "1. p(a ∩ (harry, potter, is))/p(harry, potter, is)\n",
    "1. p(wizard ∩ (harry, potter, is, a))/p(harry, potter, is, a) ... And so on....<br>\n",
    "\n",
    "The simplistic assumption of an n-gram model is that the word which will be generated at t<sup>th</sup> position depends on last n-1 words. <br>\n",
    "For example want to generate using an n-gram model with n=3. The below computations are to be done.\n",
    "1. Let seed phrase be 'thus we want harry potter .... go on generating'\n",
    "1. Truncate the seed phrase to last n-1 words to predict what will come in the blanks. i.e. truncate it to 'harry potter'.\n",
    "1. Calculate the probabilities of each word in our vocabulary as follows:\n",
    "    1. p(is ∩ (harry, potter))/p(harry, potter)<br>\n",
    "       = count(harry potter is)/count(harry potter)\n",
    "    1. p(a ∩ (potter, is))/p(potter, is)\n",
    "    1. p(wizard ∩ (his, a))/p(is, a)\n",
    "1. Calculate the highest among all the above probabilities which will denote the most probable word that will come as the next word.\n",
    "1. Stop the generation of sentences when we encounter words like '.','!' or '?'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_next_word(\n",
    "    seed_text: str,\n",
    "    text_corpus: str,\n",
    "    n_gram: int = 4\n",
    ") -> str:\n",
    "    '''\n",
    "    Find the probabilities of each word in our vocabulary to appear given the seed texts\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    seed_text : str\n",
    "        DESCRIPTION.\n",
    "    text_corpus : str\n",
    "        DESCRIPTION.\n",
    "    n_gram : int (optional)\n",
    "        N-grams to process.\n",
    "        Default value 4\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None.\n",
    "\n",
    "    '''\n",
    "    # truncate the seed text by selecting last n-1 words\n",
    "    seed_text_truncated = preprocess_seed_text(seed_text, n_gram-1)\n",
    "    print(f'Info: Truncated seed text:{seed_text_truncated}')\n",
    "    # define next word as blank\n",
    "    next_word = ''\n",
    "    # for storing the probabilities\n",
    "    probs = []\n",
    "    # search with the whole truncated seed_text\n",
    "    denominator_count = count_gram(search_phrase=seed_text_truncated,text_corpus=text_corpus)\n",
    "    if denominator_count == 0:\n",
    "        \"\"\"\n",
    "        remove first word from seed text\n",
    "        this is the fallback mechanism where if we give an n value is so large that model\n",
    "        is not able to find that combination of phrases\n",
    "        \"\"\"\n",
    "        print(f'Info: Seed text:{seed_text_truncated} not found in corpus')\n",
    "        seed_text = ' '.join(seed_text.split()[1:])\n",
    "        return seed_text\n",
    "    else:\n",
    "        # if denominator is not 0 means the phrase is found now start computing the probabilities\n",
    "        # compute the count of seed text of vocabulary word appearing after the given seed text\n",
    "        for word in vocabulary:\n",
    "            new_search_phrase = seed_text_truncated + ' ' + word\n",
    "            # compute the numerator count\n",
    "            numerator_count = count_gram(search_phrase=new_search_phrase,text_corpus=text_corpus)\n",
    "            # compute the probability of the given word\n",
    "            prob = numerator_count/denominator_count\n",
    "            probs.append(prob)\n",
    "        # find the position where the maximum probability is occuring\n",
    "        maxpos = probs.index(max(probs))\n",
    "        # give the next word with the highest probability\n",
    "        next_word = vocabulary[maxpos]\n",
    "        sentence = seed_text + ' ' + next_word\n",
    "        print(f'Info: Sentence generated:{sentence}')\n",
    "        print('---------------------------------------')\n",
    "        return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def continue_text_generation(seed_text: str,\n",
    "                             text_corpus: str,\n",
    "                             n_gram:int = 4\n",
    "                            ):\n",
    "    '''\n",
    "    Find the probabilities of each word in our vocabulary to appear given the seed texts\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    seed_text : str\n",
    "        DESCRIPTION.\n",
    "    text_corpus : str\n",
    "        DESCRIPTION.\n",
    "    n_gram : int (optional)\n",
    "        N-grams to process.\n",
    "        Default value 4\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None.\n",
    "\n",
    "    '''\n",
    "    '''\n",
    "    continue the text generation process till:\n",
    "        seed_text is present: \n",
    "            this is because if we give such combination of words such that they are not present\n",
    "            in corpus we are cutting the seed text by 1 word from the beginning. A time may reach\n",
    "            if none of the words are found and the process stops generating no new word.\n",
    "        and end of sentence is not reached:\n",
    "            This is reached if the next word are one of End of sentence characters as ['.','!','?']\n",
    "    '''\n",
    "    while seed_text != '' and seed_text.split()[-1] not in ['.','!','?']:\n",
    "        seed_text = generate_next_word(seed_text, text_corpus, n_gram)\n",
    "    print(seed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main script\n",
    "Take user input of word sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_text = 'the wizard'\n",
    "n_gram=3\n",
    "file_path = './datasets/Harry Potter and the Sorcerer.txt'\n",
    "file_text = read_file(file_path)\n",
    "cleaned_file_text = clean_text(file_text)\n",
    "vocabulary = generate_vocab(cleaned_file_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's continue the generation:\n",
    "<b><font color='red'>Warning!!: The below statement will take a very long time to process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info: Truncated seed text:the wizard\n",
      "Info: Sentence generated:the wizard ,\n",
      "---------------------------------------\n",
      "Info: Truncated seed text:wizard ,\n",
      "Info: Sentence generated:the wizard , about\n",
      "---------------------------------------\n",
      "Info: Truncated seed text:, about\n",
      "Info: Sentence generated:the wizard , about a\n",
      "---------------------------------------\n",
      "Info: Truncated seed text:about a\n",
      "Info: Sentence generated:the wizard , about a foot\n",
      "---------------------------------------\n",
      "Info: Truncated seed text:a foot\n",
      "Info: Sentence generated:the wizard , about a foot from\n",
      "---------------------------------------\n",
      "Info: Truncated seed text:foot from\n",
      "Info: Sentence generated:the wizard , about a foot from the\n",
      "---------------------------------------\n",
      "Info: Truncated seed text:from the\n",
      "Info: Sentence generated:the wizard , about a foot from the ceiling\n",
      "---------------------------------------\n",
      "Info: Truncated seed text:the ceiling\n",
      "Info: Sentence generated:the wizard , about a foot from the ceiling .\n",
      "---------------------------------------\n",
      "the wizard , about a foot from the ceiling .\n",
      "Wall time: 9min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "continue_text_generation(seed_text=seed_text, text_corpus=cleaned_file_text, n_gram=n_gram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference\n",
    "Here we can see that it is very slow in running and has a high running time complexity. Also increasing the window may cause the search to automatically limit itself to last word only. Output are not that great looking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
